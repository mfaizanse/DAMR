{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from StereoNet_single import StereoNet\n",
    "from StereoNet_single import StereoNetOnlyRefine\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from kinfu_cv import KinfuPlarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length = 643.338013 #942.8       # lense focal length, 1.88mm, 942.8 ???  643.338013, 643.096008\n",
    "baseline = 55   #49.75  distance in mm between the two cameras\n",
    "units = 0.512     # depth units, adjusted for the output to fit in one byte\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "# intrinsicsNp = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "## FocalLengthLeft: 643.338379 643.096497\n",
    "## PrincipalPointLeft: 638.957336 402.329956\n",
    "## PinholeCameraIntrinsic(width: int, height: int, fx: float, fy: float, cx: float, cy: float)\n",
    "pinhole_camera_intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, \n",
    "                                                             643.338013, 643.096008, \n",
    "                                                             638.956970, 402.330017);\n",
    "\n",
    "print('Camera intrinsics:')\n",
    "print(pinhole_camera_intrinsic.intrinsic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDisparityMapToDepthMap(disparityMap):\n",
    "    # shape: disparityMap.shape\n",
    "    valid_pixels = disparityMap > 0\n",
    "    depth = np.zeros(shape=disparityMap.shape).astype(\"uint16\")\n",
    "    depth[valid_pixels] = (focal_length * baseline) / (units * disparityMap[valid_pixels])\n",
    "    \n",
    "    return depth\n",
    "\n",
    "def convertDepthMapToDisparityMap(depthMap):\n",
    "    # shape: depthMap.shape\n",
    "    valid_pixels = depthMap > 0\n",
    "    disparity = np.zeros(shape=depthMap.shape).astype(\"float32\")\n",
    "    disparity[valid_pixels] = (focal_length * baseline) / (units * depthMap[valid_pixels])\n",
    "    \n",
    "    return disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./data/checkpoint_pretrain_secneflow.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "stereoNetModel = StereoNet(k=4-1, r=4-1, maxdisp=192)\n",
    "stereoNetModel = nn.DataParallel(stereoNetModel)\n",
    "stereoNetModel.load_state_dict(checkpoint['state_dict'])\n",
    "stereoNetModel.eval();\n",
    "\n",
    "stereoNetOnlyRefineModel = StereoNetOnlyRefine(k=4-1, r=4-1, maxdisp=192)\n",
    "stereoNetOnlyRefineModel = nn.DataParallel(stereoNetOnlyRefineModel)\n",
    "stereoNetOnlyRefineModel.load_state_dict(checkpoint['state_dict'])\n",
    "stereoNetOnlyRefineModel.eval();\n",
    "\n",
    "def convertImgToModelInput2(img):\n",
    "    normalize = {'mean': [0.0, 0.0, 0.0], 'std': [1.0, 1.0, 1.0]}\n",
    "    \n",
    "    imgTmp = img\n",
    "    imgTmp = imgTmp.astype(float)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**normalize)\n",
    "    ])\n",
    "    \n",
    "    imgTmp = transform(imgTmp).float()\n",
    "    return imgTmp\n",
    "\n",
    "def convertImgToModelInput(img):\n",
    "    normalize = {'mean': [0.0, 0.0, 0.0], 'std': [1.0, 1.0, 1.0]}\n",
    "    \n",
    "    imgTmp = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    imgTmp = imgTmp.astype(float)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**normalize)\n",
    "    ])\n",
    "    \n",
    "    imgTmp = transform(imgTmp).float()\n",
    "    return imgTmp\n",
    "\n",
    "def getStereoNet(left, right, useRefinedDisparity = True):\n",
    "#     normalize = {'mean': [0.0, 0.0, 0.0], 'std': [1.0, 1.0, 1.0]}\n",
    "#     m = left.shape[0] # 480\n",
    "#     n = right.shape[1] # 640\n",
    "\n",
    "    imgL = convertImgToModelInput(left)\n",
    "    imgR = convertImgToModelInput(right)\n",
    "    \n",
    "#     imgL = cv2.cvtColor(left, cv2.COLOR_GRAY2RGB)\n",
    "#     imgR = cv2.cvtColor(right, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "#     imgL = imgL.astype(float)\n",
    "#     imgR = imgR.astype(float)\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(**normalize)\n",
    "#     ])\n",
    "\n",
    "#     imgL = transform(imgL).float()\n",
    "#     imgR = transform(imgR).float()\n",
    "\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        imgL1 = imgL.unsqueeze(0)\n",
    "        imgR1 = imgR.unsqueeze(0)\n",
    "        outputs = stereoNetModel(imgL1, imgR1)\n",
    "\n",
    "#         print('model output')\n",
    "#         print(len(outputs))\n",
    "#         print(outputs[0].shape)\n",
    "#         print(outputs[1].shape)\n",
    "\n",
    "    sn_disparity = outputs[0].squeeze(0).cpu().numpy()\n",
    "    sn_disparity_refined = outputs[1].squeeze(0).cpu().numpy()\n",
    "    \n",
    "    disparity_to_use = sn_disparity\n",
    "    if useRefinedDisparity:\n",
    "        #disparity_to_use = disparity_to_use + sn_disparity_refined\n",
    "        disparity_to_use = sn_disparity_refined\n",
    "    \n",
    "    # convert to depth map\n",
    "    sn_depth_map = convertDisparityMapToDepthMap(disparity_to_use)\n",
    "    sn_depth_map[disparity_to_use < 0] = 0\n",
    "\n",
    "    return sn_depth_map, sn_disparity\n",
    "\n",
    "def getRefinedDepthMap(depth, left):\n",
    "    rs_disparityMap = convertDepthMapToDisparityMap(depth) # convert depth to disparity\n",
    "    rs_disparityMap_tensor = torch.tensor(rs_disparityMap).float()\n",
    "    \n",
    "    imgL = convertImgToModelInput2(left)    \n",
    "    output1 = None\n",
    "    with torch.no_grad():\n",
    "        imgL1 = imgL.unsqueeze(0)\n",
    "        rs_disparityMap_tensor = rs_disparityMap_tensor.unsqueeze(0)\n",
    "        output1 = stereoNetOnlyRefineModel(rs_disparityMap_tensor, imgL1)\n",
    "        \n",
    "    refined_rs_disparity = output1.squeeze(0).cpu().numpy()\n",
    "    refined_rs_depth = convertDisparityMapToDepthMap(refined_rs_disparity)\n",
    "    \n",
    "    return refined_rs_depth, refined_rs_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPointCloud(rgb_img, depth_image):\n",
    "    # expects color image to be 8-bits per pixel\n",
    "    # expects depth image to be 16-bits per pixel\n",
    "    \n",
    "    o3dColorImage1 = o3d.geometry.Image(rgb_img)\n",
    "    o3dDepthImage1 = o3d.geometry.Image(depth_image)\n",
    "\n",
    "    # Create an RGBD open3d Image\n",
    "    frame_rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(o3dColorImage1, o3dDepthImage1, \n",
    "                                                                          convert_rgb_to_intensity=False)\n",
    "\n",
    "    # Create Point cloud\n",
    "    framePC = o3d.geometry.PointCloud.create_from_rgbd_image(frame_rgbd_image, pinhole_camera_intrinsic)\n",
    "    \n",
    "    \n",
    "#   framePC = o3d.geometry.PointCloud.create_from_depth_image(o3dDepthImage1, pinhole_camera_intrinsic, \n",
    "#                                                                   depth_scale=1000.0)\n",
    "    \n",
    "    return framePC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "res_x = width\n",
    "res_y = height\n",
    "\n",
    "config.enable_stream(rs.stream.depth, res_x, res_y, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, res_x, res_y, rs.format.bgr8, 30)\n",
    "\n",
    "config.enable_stream(rs.stream.infrared, 1, res_x, res_y, rs.format.y8, 30)\n",
    "config.enable_stream(rs.stream.infrared, 2, res_x, res_y, rs.format.y8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline_profile = pipeline.start(config)\n",
    "\n",
    "## To set laser on/off or set laser power\n",
    "device = pipeline_profile.get_device()\n",
    "depth_sensor = device.query_sensors()[0]\n",
    "laser_pwr = depth_sensor.get_option(rs.option.laser_power)\n",
    "print(\"laser power = \", laser_pwr)\n",
    "laser_range = depth_sensor.get_option_range(rs.option.laser_power)\n",
    "print(\"laser power range = \" , laser_range.min , \"~\", laser_range.max)\n",
    "depth_sensor.set_option(rs.option.laser_power, 0)\n",
    "\n",
    "f1 = pipeline.wait_for_frames()\n",
    "d1 = f1.get_depth_frame()\n",
    "\n",
    "\n",
    "#// Depth scale is needed for the kinfu_plarr set-up\n",
    "depth_sensor = pipeline_profile.get_device().first_depth_sensor()\n",
    "\n",
    "# depth_sensor.set_option(rs.option.visual_preset, rs.option.RS400_VISUAL_PRESET_HIGH_DENSITY);\n",
    "preset_range = depth_sensor.get_option_range(rs.option.visual_preset)\n",
    "print('preset range:'+str(preset_range))\n",
    "\n",
    "for i in range(int(preset_range.max)):\n",
    "    visulpreset = depth_sensor.get_option_value_description(rs.option.visual_preset,i)\n",
    "    print(i, visulpreset)\n",
    "    \n",
    "    if visulpreset == \"High Accuracy\":\n",
    "        depth_sensor.set_option(rs.option.visual_preset, i)\n",
    "\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "print(\"Depth Scale is: \" , depth_scale)\n",
    "\n",
    "\n",
    "spatial = rs.spatial_filter()\n",
    "spatial.set_option(rs.option.filter_magnitude, 3)\n",
    "spatial.set_option(rs.option.filter_smooth_alpha, 0.5)\n",
    "spatial.set_option(rs.option.filter_smooth_delta, 20)\n",
    "spatial.set_option(rs.option.holes_fill, 0)\n",
    "temporal = rs.temporal_filter()\n",
    "# decimation = rs.decimation_filter()\n",
    "# decimation.set_option(rs.option.filter_magnitude, 4)\n",
    "hole_filling = rs.hole_filling_filter()\n",
    "hole_filling.set_option(rs.option.holes_fill, 1)\n",
    "\n",
    "# Image directory \n",
    "directory = r'./outputs/'\n",
    "img_count = 1\n",
    "\n",
    "kfp = KinfuPlarr(width, height, depth_scale, 643.338013, 643.096008, 638.95697, 402.330017, True)\n",
    "\n",
    "rayCastedDepth = np.zeros(shape=(height, width))\n",
    "\n",
    "cc = 0\n",
    "try:\n",
    "    while True:\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        ir1_frame = frames.get_infrared_frame(1) # Left IR Camera, it allows 0, 1 or no input\n",
    "        ir2_frame = frames.get_infrared_frame(2) # Right IR camera\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        if not ir1_frame or not ir2_frame:\n",
    "            continue\n",
    "            \n",
    "        # skipping first 20 frames, to wait for sensor for warm up\n",
    "        cc = cc + 1\n",
    "        if cc < 20:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data()) # dtype of print(depth_image.dtype) is: uint16\n",
    "        color_image = np.asanyarray(color_frame.get_data()) # FORMAT: BGR, dtype of print(color_image.dtype) is: uint8\n",
    "        color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ir1_image = np.asanyarray(ir1_frame.get_data()) # Left image\n",
    "        ir2_image = np.asanyarray(ir2_frame.get_data()) # Right image\n",
    "        \n",
    "        left = ir1_image\n",
    "        right = ir2_image\n",
    "        rs_depth = depth_image # (image of 16-bit per pixel)\n",
    "        \n",
    "        ## Apply filters to real-sense depth\n",
    "        filtered_rs_depth = spatial.process(depth_frame)\n",
    "        filtered_rs_depth = temporal.process(filtered_rs_depth)\n",
    "        filtered_rs_depth = hole_filling.process(filtered_rs_depth)\n",
    "        filtered_rs_depth = np.asanyarray(filtered_rs_depth.get_data()) # dtype of print(filtered_rs_depth.dtype) is: uint16\n",
    "        \n",
    "#         ## Apply bilateral filter to RS depth image BEFORE RS filters\n",
    "#         tmp = rs_depth.astype(np.float32)\n",
    "#         tmp = cv2.bilateralFilter(tmp, 9, 75, 75)\n",
    "#         bilateral_depth = tmp.astype(np.uint16)\n",
    "        \n",
    "#         ## Apply bilateral filter to RS depth image AFTER RS filters\n",
    "#         tmp = filtered_rs_depth.astype(np.float32)\n",
    "#         tmp = cv2.bilateralFilter(tmp, 9, 75, 75)\n",
    "#         filtered_bilateral_depth = tmp.astype(np.uint16)\n",
    "        \n",
    "        \n",
    "#         print('# of valid depth values [rs_depth]: ', len(rs_depth[rs_depth > 0]))\n",
    "#         print(rs_depth[rs_depth > 0])\n",
    "#         print('# of valid depth values [bilateral_depth]: ', len(bilateral_depth[bilateral_depth > 0]))\n",
    "#         print(bilateral_depth[bilateral_depth > 0])\n",
    "        \n",
    "#         print('# of valid depth values [filtered_rs_depth]: ', len(filtered_rs_depth[filtered_rs_depth > 0]))\n",
    "#         print(filtered_rs_depth[filtered_rs_depth > 0])\n",
    "#         print('# of valid depth values [filtered_bilateral_depth]: ', len(filtered_bilateral_depth[filtered_bilateral_depth > 0]))\n",
    "#         print(filtered_bilateral_depth[filtered_bilateral_depth > 0])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        ## Use StereoNet to estimate depth\n",
    "        sn_depth_map, sn_disparity = getStereoNet(left, right, False)\n",
    "        \n",
    "    \n",
    "#         print('StereoNet Disparitymap')\n",
    "#         print(sn_disparity.shape, sn_disparity.dtype)\n",
    "        \n",
    "#         print('StereoNet Depthmap')\n",
    "#         print(sn_depth_map.shape, sn_depth_map.dtype)\n",
    "        \n",
    "#         print('# of valid depth values [sn_depth_map]: ', len(sn_depth_map[sn_depth_map > 0]))\n",
    "#         print(sn_depth_map[sn_depth_map > 0])\n",
    "\n",
    "\n",
    "        ## Use Edge Refinement Network to refine depth from RealSense\n",
    "        #refined_rs_depth, refined_rs_disparity = getRefinedDepthMap(bilateral_depth, color_image_rgb)\n",
    "        \n",
    "        ## Kinect Fusion\n",
    "#         depth_image_flatten = np.array(depth_image).flatten()\n",
    "#         depth_image_flatten = depth_image_flatten.tolist()\n",
    "#         isSuccess = kfp.integrateFrame(depth_image_flatten)\n",
    "#         if isSuccess:\n",
    "#             pose = kfp.getPose()\n",
    "#             pose = np.asarray(pose, dtype=np.float32, order='C').reshape((4, -1))\n",
    "# #             print(pose.shape)\n",
    "# #             print(pose.dtype)\n",
    "#             print(pose)\n",
    "            \n",
    "#             d = kfp.getCurrentDepth()\n",
    "#             d = np.array(d)\n",
    "#             d = np.nan_to_num(d)\n",
    "#             d = d.reshape((height, -1))\n",
    "#             d = d * (1/depth_scale)\n",
    "#             d = d.astype(np.int16)\n",
    "            \n",
    "#             rayCastedDepth = d\n",
    "            \n",
    "#             kfp.renderShow()\n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        #### VISUALISATION ####\n",
    "        is_visual_on = True\n",
    "        if not is_visual_on:\n",
    "            continue\n",
    "        \n",
    "        ## Visualize RGB frame\n",
    "#         cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "#         cv2.imshow('RealSense', color_image)\n",
    "        \n",
    "        ## Visualize IR frames\n",
    "        ir_images = np.hstack((ir1_image, ir2_image))\n",
    "        cv2.namedWindow('IRSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('IRSense', ir_images)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         ## Visualize Real sense depth frames\n",
    "#         ## Apply colormap on Real-sense depth image \n",
    "#         rs_depth_sccaled  = cv2.convertScaleAbs(depth_image, alpha=0.03)  # (image converted to 8-bit per pixel)\n",
    "#         filtered_rs_depth_scaled = cv2.convertScaleAbs(filtered_rs_depth, alpha=0.03)  # (image converted to 8-bit per pixel)\n",
    "#         #depth_colormap = cv2.applyColorMap(rs_depth_sccaled, cv2.COLORMAP_JET)\n",
    "#         depth_colormap = cv2.applyColorMap(cv2.equalizeHist(rs_depth_sccaled), cv2.COLORMAP_JET)\n",
    "#         #filtered_depth_colormap = cv2.applyColorMap(filtered_rs_depth_scaled, cv2.COLORMAP_JET)\n",
    "#         filtered_depth_colormap = cv2.applyColorMap(cv2.equalizeHist(filtered_rs_depth_scaled), cv2.COLORMAP_JET)\n",
    "#         both_depths2 = np.hstack((depth_colormap, filtered_depth_colormap))\n",
    "#         cv2.namedWindow('RS_DepthMap', cv2.WINDOW_AUTOSIZE)\n",
    "#         cv2.imshow('RS_DepthMap', both_depths2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Visualize Real sense depth frames with Bilateral filter\n",
    "#         bilateral_depth_colormap = cv2.applyColorMap(cv2.equalizeHist(cv2.convertScaleAbs(bilateral_depth, alpha=0.03)), cv2.COLORMAP_JET)\n",
    "#         bilateral_filtered_depth_colormap = cv2.applyColorMap(cv2.equalizeHist(cv2.convertScaleAbs(filtered_bilateral_depth, alpha=0.03)), cv2.COLORMAP_JET)\n",
    "#         both_depths2 = np.hstack((bilateral_depth_colormap, bilateral_filtered_depth_colormap))\n",
    "#         cv2.namedWindow('RS_Bilateral_DepthMap', cv2.WINDOW_AUTOSIZE)\n",
    "#         cv2.imshow('RS_Bilateral_DepthMap', both_depths2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #         ## Visualize rayCastedDepth\n",
    "#         tmp1 = cv2.convertScaleAbs(rayCastedDepth, alpha=0.03)\n",
    "#         rayCastedDepth_colormap = cv2.applyColorMap(cv2.equalizeHist(tmp1), cv2.COLORMAP_JET)\n",
    "#         cv2.namedWindow('rayCastedDepth', cv2.WINDOW_AUTOSIZE)\n",
    "#         cv2.imshow('rayCastedDepth', rayCastedDepth_colormap)\n",
    "        \n",
    "        \n",
    "        ## Visualize stereoNet disparity and depth maps (StereoNET)\n",
    "        tmp1 = cv2.convertScaleAbs(sn_disparity.astype(np.uint16))\n",
    "        o1 = cv2.applyColorMap(cv2.equalizeHist(tmp1), cv2.COLORMAP_JET)\n",
    "        o1[sn_disparity < 0] = 0\n",
    "        o2 = cv2.applyColorMap(cv2.equalizeHist(cv2.convertScaleAbs(sn_depth_map, alpha=0.03)), cv2.COLORMAP_JET)\n",
    "        o2[sn_depth_map < 0] = 0\n",
    "        #stn_images = o1\n",
    "        stn_images = np.hstack((o1, o2))\n",
    "        cv2.namedWindow('StereoNet', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('StereoNet', stn_images)\n",
    "        \n",
    "        \n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        # Press esc or 'q' to close the image window\n",
    "        if key & 0xFF == ord('q') or key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        if key == 116:\n",
    "#             cv2.imwrite(directory + str(img_count) + '_color_image.jpg', color_image)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_left.jpg', left)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_right.jpg', right)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_rs_depth.jpg', depth_colormap)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_rs_filtered_depth.jpg', filtered_depth_colormap)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_bm_depth.jpg', temp1)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_bm_filtered_depth.jpg', temp2)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_bm_disparity.jpg', raw_disparity_scaled)\n",
    "#             cv2.imwrite(directory + str(img_count) + '_bm_filtered_disparity.jpg', filtered_disparity_scaled)\n",
    "            img_count = img_count+1\n",
    "        if key == ord('1'):\n",
    "            rs_pc = getPointCloud(color_image_rgb, rs_depth)\n",
    "            rs_pc.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "            print('rs_pc', rs_pc)\n",
    "            o3d.visualization.draw_geometries([rs_pc])\n",
    "        if key == ord('2'):\n",
    "            rs_pc = getPointCloud(color_image_rgb, filtered_rs_depth)\n",
    "            rs_pc.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "            print('rs_pc', rs_pc)\n",
    "            o3d.visualization.draw_geometries([rs_pc])\n",
    "#         if key == ord('3'):\n",
    "#             rs_pc = getPointCloud(color_image_rgb, bilateral_depth)\n",
    "#             rs_pc.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "#             print('rs_pc', rs_pc)\n",
    "#             o3d.visualization.draw_geometries([rs_pc])\n",
    "        if key == ord('4'):\n",
    "            rs_pc = getPointCloud(color_image_rgb, sn_depth_map)\n",
    "            rs_pc.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "            print('rs_pc', rs_pc)\n",
    "            o3d.visualization.draw_geometries([rs_pc])\n",
    "        if key == ord('5'):\n",
    "            rs_pc = getPointCloud(color_image_rgb, rayCastedDepth)\n",
    "            rs_pc.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "            print('rs_pc', rs_pc)\n",
    "            o3d.visualization.draw_geometries([rs_pc])\n",
    "        if key == ord('t'):\n",
    "            cv2.imwrite(directory + str(img_count) + '_color_image.jpg', color_image)\n",
    "            cv2.imwrite(directory + str(img_count) + '_left.jpg', left)\n",
    "            cv2.imwrite(directory + str(img_count) + '_right.jpg', right)\n",
    "            cv2.imwrite(directory + str(img_count) + '_rs_depth.jpg', depth_colormap)\n",
    "            cv2.imwrite(directory + str(img_count) + '_rs_filtered_depth.jpg', filtered_depth_colormap)\n",
    "            cv2.imwrite(directory + str(img_count) + '_raycasted_depth.jpg', rayCastedDepth_colormap)\n",
    "            img_count = img_count+1\n",
    "        if key == ord('y'):\n",
    "            cv2.imwrite(directory + str(img_count) + '_color_image.jpg', color_image)\n",
    "            cv2.imwrite(directory + str(img_count) + '_left.jpg', left)\n",
    "            cv2.imwrite(directory + str(img_count) + '_right.jpg', right)\n",
    "            cv2.imwrite(directory + str(img_count) + '_stereonet_depth.jpg', o2)\n",
    "            img_count = img_count+1\n",
    "            \n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
